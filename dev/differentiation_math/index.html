<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Differentiation Details · NaturalNeighbours.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://DanielVandH.github.io/NaturalNeighbours.jl/differentiation_math/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NaturalNeighbours.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../interpolation/">Interpolation</a></li><li><a class="tocitem" href="../differentiation/">Differentiaton</a></li><li><a class="tocitem" href="../swiss/">Switzerland Elevation Data</a></li></ul></li><li><a class="tocitem" href="../compare/">Comparison of Interpolation Methods</a></li><li><span class="tocitem">Mathematical Details</span><ul><li><a class="tocitem" href="../interpolation_math/">Interpolation Details</a></li><li class="is-active"><a class="tocitem" href>Differentiation Details</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Generation-at-Data-Sites"><span>Generation at Data Sites</span></a></li><li><a class="tocitem" href="#Direct-Generation"><span>Direct Generation</span></a></li><li><a class="tocitem" href="#Iterative-Generation"><span>Iterative Generation</span></a></li><li class="toplevel"><a class="tocitem" href="#Generation-Away-from-the-Data-Sites"><span>Generation Away from the Data Sites</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Mathematical Details</a></li><li class="is-active"><a href>Differentiation Details</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Differentiation Details</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/DanielVandH/NaturalNeighbours.jl/blob/main/docs/src/differentiation_math.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Differentiation"><a class="docs-heading-anchor" href="#Differentiation">Differentiation</a><a id="Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Differentiation" title="Permalink"></a></h1><p>In this section, we give some of the mathematical detail used for implementing derivative generation, following this <a href="https://kluedo.ub.rptu.de/frontdoor/deliver/index/docId/2104/file/diss.bobach.natural.neighbor.20090615.pdf">thesis</a>. The discussion that follows is primarily sourced from Chapter 6 of the linked thesis. While it is possible to generate derivatives of arbitary order, our discussion here in this section will be limited to gradient and Hessian generation.  These ideas are implemented by the <code>generate_gradients</code> and <code>generate_derivatives</code> functions, which you should use via the <code>differentiate</code> function.</p><h1 id="Generation-at-Data-Sites"><a class="docs-heading-anchor" href="#Generation-at-Data-Sites">Generation at Data Sites</a><a id="Generation-at-Data-Sites-1"></a><a class="docs-heading-anchor-permalink" href="#Generation-at-Data-Sites" title="Permalink"></a></h1><p>Let us first consider generating derivatives at the data points used to define the interpolant, <span>$(\boldsymbol x_1, z_1), \ldots, (\boldsymbol x_n, z_n)$</span>. We consider generating the derivatives at a data site <span>$\boldsymbol x_0$</span>, where <span>$\boldsymbol x_0$</span> is some point in <span>$(\boldsymbol x_1,\ldots,\boldsymbol x_n)$</span> so that we also know <span>$z_0$</span>.</p><h2 id="Direct-Generation"><a class="docs-heading-anchor" href="#Direct-Generation">Direct Generation</a><a id="Direct-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Direct-Generation" title="Permalink"></a></h2><p>Let us consider a direct approach first. In this approach, we generate gradients and Hessians jointly. We approximate the underlying function <span>$f$</span> by a Taylor series expansion,</p><p class="math-container">\[\tilde f(\boldsymbol x) = z_0 + \tilde f_1(\boldsymbol x) + \tilde f_2(\boldsymbol x) + \tilde f_3(\boldsymbol x),\]</p><p>where </p><p class="math-container">\[\begin{align*}
\tilde f_1(\boldsymbol x) &amp;= \frac{\partial f(\boldsymbol x_0)}{\partial x}(x-x_0) + \frac{\partial f(\boldsymbol x_0)}{\partial y}(y - y_0), \\
\tilde f_2(\boldsymbol x) &amp;= \frac12\frac{\partial^2 f(\boldsymbol x_0)}{\partial x^2}(x - x_0)^2 + \frac12{\partial^2 f(\boldsymbol x_0)}{\partial y^2}(y - y_0)^2 + \frac{\partial^2 f(\boldsymbol x_0)}{\partial x\partial y}(x-x_0)(y-y_0), \\
\tilde f_3(\boldsymbol x) &amp;= \frac16\frac{\partial^3 f(\boldsymbol x_0)}{\partial x^3}(x-x_0)^3 + \frac16\frac{\partial^3 f(\boldsymbol x_0)}{\partial y^3}(y-y_0)^3 \\&amp;+ \frac12\frac{\partial^3 f(\boldsymbol x_0)}{\partial x^2\partial y}(x-x_0)^2(y-y_0) + \frac12\frac{\partial^3 f(\boldsymbol x_0)}{\partial x\partial y^2}(x-x_0)(y-y_0)^2.
\end{align*}\]</p><p>For gradient generation only, we need only take up to <span>$\tilde f_1$</span>, but for Hessian generation we could include up to <span>$\tilde f_2$</span> or up to <span>$\tilde f_3$</span>. Whatever option we choose, the neighbourhood that we use for approximating the derivatives needs to be chosen to match the order of the approximation.</p><p>To choose the neighbourhood, define the <span>$d$</span>-times iterated neighbourhood of <span>$\boldsymbol x_0$</span> by </p><p class="math-container">\[N_0^d = \bigcup_{i \in N_0^{d-1}} N_i \setminus \{0\}, \quad N_0^1 = N_0.\]</p><p>Here, the neighbourhoods are the <em>Delaunay neighbourhoods</em>, not the natural neighbours – for points <span>$\boldsymbol x_0$</span> that are not one of the existing data sites, natural neighbours are used instead. An example of <span>$N_0^1$</span> and <span>$N_0^2$</span> both at a data site and away from a data site is shown below, where <span>$\boldsymbol x_0$</span> is shown in magenta, <span>$N_0^1$</span> in blue, and <span>$N_0^2$</span> in red (and also includes the blue points).</p><figure>
    <img src='../figures/taylor_neighbourhood.png', alt'Iterated neighbourhood examples'><br>
</figure><h3 id="Gradients"><a class="docs-heading-anchor" href="#Gradients">Gradients</a><a id="Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Gradients" title="Permalink"></a></h3><p>Let&#39;s now use the notation defined above to define how gradients are generated in <code>generate_derivatives</code>, without having to estimate Hessians at the same time. The neighbourhood we use is <span>$N_0^1$</span>, and we take <span>$\tilde f = z_0 + \tilde f_1$</span>. We define the following weighted least squares problem for the estimates <span>$\beta_x$</span>, <span>$\beta_y$</span> of <span>$\partial f(\boldsymbol x_0)/\partial x$</span> and <span>$\partial f(\boldsymbol x_0)/\partial y$</span>, respectively:</p><p class="math-container">\[(\beta_x, \beta_y) = \text{argmin}_{(\beta_x, \beta_y)} \sum_{i \in \mathcal N_0^1} W_i \left(\tilde z_i - \beta_1\tilde x_i - \beta_2\tilde y_i\right)^2,\]</p><p>where <span>$W_i = 1/\|\boldsymbol x_i - \boldsymbol x_i\|^2$</span>, <span>$\tilde z_i = z_i-z_0$</span>, <span>$\tilde x_i=x_i-x_0$</span>, and <span>$\tilde y_i = y_i-y_0$</span>. This weighted least squares problem is solved by solving the associated linear system <span>$\tilde{\boldsymbol X}\boldsymbol{\beta} = \tilde{\boldsymbol z}$</span>, where <span>$\tilde{\boldsymbol X} \in \mathbb R^{m \times 2}$</span> is defined by <span>$(\tilde{\boldsymbol X})_{i1} = \sqrt{W_i}(x_i - x_0)$</span> and <span>$(\tilde{\boldsymbol X})_{i2} = \sqrt{W_i}(y_i - y_0)$</span>, <span>$\boldsymbol{\beta} = (\beta_1,\beta_2)^T$</span>, and <span>$\tilde{\boldsymbol z} = (\tilde z_1,\ldots,\tilde z_m)^T$</span>.</p><h3 id="Joint-Gradients-and-Hessians"><a class="docs-heading-anchor" href="#Joint-Gradients-and-Hessians">Joint Gradients and Hessians</a><a id="Joint-Gradients-and-Hessians-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-Gradients-and-Hessians" title="Permalink"></a></h3><p>Hessians can similarly be estimated, although currently they must be estimated jointly with gradients. We take <span>$\tilde f = z_0 + \tilde f_1 + \tilde f_2$</span> in the following discussion, although taking up to <span>$\tilde f_3$</span> has an obvious extension. (The reason to also allow for estimating up to the cubic terms is because sometimes it provides better estimates for the Hessians than only going up to the quadratic terms – see the examples in Chapter 6 <a href="https://kluedo.ub.rptu.de/frontdoor/deliver/index/docId/2104/file/diss.bobach.natural.neighbor.20090615.pdf">here</a>.) </p><p>Defining <span>$\beta_1 = \partial f(\boldsymbol x_0)/\partial x$</span>, <span>$\beta_2 = \partial f(\boldsymbol x_0)/\partial y$</span>, <span>$\beta_3 = \partial^2 f(\boldsymbol x_0)/\partial x^2$</span>, <span>$\beta_4 = \partial^2 f(\boldsymbol x_0)/\partial y^2$</span>, and <span>$\beta_5 = \partial^2 f(\boldsymbol x_0)/\partial x\partial y$</span>, we have the following weighted least squares problem with <span>$\boldsymbol{\beta}=(\beta_1,\beta_2,\beta_3,\beta_4,\beta_5)^T$</span>:</p><p class="math-container">\[\boldsymbol{\beta} = \text{argmin}_{\boldsymbol{\beta}} \sum_{i \in N_0^2} W_i\left(\tilde z_i - \beta_1\tilde x_i - \beta_2\tilde y_i - \frac12\beta_3\tilde x_i^2 - \frac12\beta_4\tilde y_i^2 - \beta_5\tilde x_i\tilde y_i\right)^2,\]</p><p>using similar notation as in the gradient case. (In the cubic case, use <span>$N_0^3$</span> and go up to <span>$\beta_9$</span>, discarding <span>$\beta_6,\ldots,\beta_9$</span> at the end.) The associated linear system in this case has matrix <span>$\tilde{\boldsymbol X} \in \mathbb R^{m \times 2}$</span> (<span>$m = |N_0^2|$</span>) defined by <span>$(\tilde{\boldsymbol X})_{i1} = \sqrt{W_i}\tilde x_i$</span>, <span>$(\tilde{\boldsymbol X})_{i2} = \sqrt{W_i}\tilde y_i$</span>, <span>$(\tilde{\boldsymbol X})_{i3} = \sqrt{W_i}\tilde x_i^2$</span>, <span>$(\tilde{\boldsymbol X})_{i4} = \sqrt{W_i}\tilde y_i^2$</span>, and <span>$(\tilde{\boldsymbol X})_{i5} = \sqrt{W_i}\tilde x_i\tilde y_i$</span>.</p><h2 id="Iterative-Generation"><a class="docs-heading-anchor" href="#Iterative-Generation">Iterative Generation</a><a id="Iterative-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Iterative-Generation" title="Permalink"></a></h2><p>Now we discuss iterative generation. Here, we suppose that we have already estimated gradients at all of the data sites <span>$\boldsymbol x_i$</span> neighbouring <span>$\boldsymbol x_0$</span> using the direct approach. To help with the notation, we will let <span>$\boldsymbol g_i^1$</span> denote our initial estimate of the gradient at a point <span>$\boldsymbol x_i$</span>, and the gradient and Hessian that we are now estimating at <span>$\boldsymbol x_0$</span> are given by <span>$\boldsymbol g_0^2$</span> and <span>$\boldsymbol H_0^2$</span>, respectively.</p><p>We define the following loss function, where <span>$\beta_i = 1/\|\boldsymbol x_i-\boldsymbol x_0\|$</span> and <span>$\alpha \in (0, 1)$</span>:</p><p class="math-container">\[\begin{align*}
\mathcal L(\boldsymbol g_0^2, \boldsymbol H_0^2) &amp;= \sum_{i \in \mathcal N_0} W_i\left[\alpha \mathcal L_1^i(\boldsymbol g_0^2, \boldsymbol H_0^2)^2 + (1-\alpha)L_2^i(\boldsymbol g_0^2, \boldsymbol H_0^2)^2\right], \\
\mathcal L_1^i(\boldsymbol g_0^2, \boldsymbol H_0^2)^2 &amp;= \left[\frac12(\boldsymbol x_i-\boldsymbol x_0)^T\boldsymbol H_0^2(\boldsymbol x_i - \boldsymbol x_0) + (\boldsymbol x_i-\boldsymbol x_0)^T\boldsymbol g_0^2 + z_0-z_i\right]^2, \\
\mathcal L_2^i(\boldsymbol g_0^2, \boldsymbol H_0^2) &amp;= \left\|\boldsymbol H_0^2 \boldsymbol x_i + \boldsymbol g_0^2 - \boldsymbol g_i^1\right\|^2.
\end{align*}\]</p><p>This objective function combines the losses between <span>$\tilde f(\boldsymbol x_i)$</span> and <span>$z_i$</span>, and between <span>$\boldsymbol \nabla \tilde f(\boldsymbol x_i)$</span> and <span>$\boldsymbol g_i^1$</span>, weighting them by some parameter <span>$\alpha \in (0, 1)$</span> (typically <span>$\alpha \approx 0.1$</span> is a reasonable default). After some basic algebra and calculus,  it is possible to show that minimising <span>$\mathcal L$</span> is the same as solving </p><p class="math-container">\[\overline{\boldsymbol A}^T\overline{\boldsymbol w} + \overline{\boldsymbol B}^T\overline{\boldsymbol g}_1 + \overline{\boldsymbol C}^T\overline{\boldsymbol g}_2 = \left(\overline{\boldsymbol A}^T\overline{\boldsymbol A} + \overline{\boldsymbol B}^T\overline{\boldsymbol B} + \overline{\boldsymbol C}^T\overline{\boldsymbol C}\right)\boldsymbol \theta,\]</p><p>where we define:</p><p class="math-container">\[\begin{align*}
\tilde z_i &amp;= z_i - z_0, \\
W_i &amp;= \frac{1}{\|\boldsymbol x_i-\boldsymbol x_0\|^2},\\
\gamma_i &amp;= \sqrt{\frac{\alpha}{W_i}}, \\
\gamma_i^\prime &amp;= \sqrt{\frac{1-\alpha}{W_i}},\\
\overline{\boldsymbol A}_{i,:} &amp;= \gamma_i \begin{bmatrix} x_i-x_0 &amp; y_i-y_0 &amp; \frac12(x_i-x_0)^2 &amp; \frac12(y_i-y_0)^2 &amp; (x_i-x_0)(y_i-y_0) \end{bmatrix}, \\
\overline{\boldsymbol B}_{i, :} &amp;= \gamma_i^\prime \begin{bmatrix} 1 &amp; 0 &amp; x_i - x_0 &amp; 0 &amp; y_i - y_0 \end{bmatrix}, \\
\overline{\boldsymbol C}_{i, :} &amp;= \gamma_i^\prime \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; y_i-y_0 &amp; x_i-x_0 \end{bmatrix}, \\
\overline{\boldsymbol w} &amp;= \gamma_i \tilde z_i, \\
\overline{\boldsymbol g}_1 &amp;= \gamma_i^\prime g_{i1}, \\
\overline{\boldsymbol g}_2 &amp;= \gamma_i^\prime g_{i2}, \\
\boldsymbol{\bm\theta} &amp;= \begin{bmatrix} \frac{\partial f(\boldsymbol x_0)}{\partial x} &amp; \frac{\partial f(\boldsymbol x_0)}{\partial y} &amp; \frac{\partial^2 f(\boldsymbol x_0)}{\partial x^2} &amp; \frac{\partial f(\boldsymbol x_0)}{\partial y^2} &amp; \frac{\partial f(\boldsymbol x_0)}{\partial x\partial y} \end{bmatrix}^T.
\end{align*}\]</p><p>To solve this linear system, let</p><p class="math-container">\[\boldsymbol D = \begin{bmatrix} \overline{\boldsymbol A} \\ \overline{\boldsymbol B} \\ \overline{\boldsymbol C} \end{bmatrix}, \quad \boldsymbol c = \begin{bmatrix} \overline{\boldsymbol w} \\ \overline{\boldsymbol g}_1 \\ \overline{\boldsymbol g}_2 \end{bmatrix},\]</p><p>so that <span>$\boldsymbol D^T\boldsymbol D\boldsymbol\theta = \boldsymbol D^T\boldsymbol c$</span>. These are just the normal equations for <span>$\boldsymbol D\boldsymbol \theta = \boldsymbol c$</span>, thus we can estimate the gradients and Hessians by simply solving <span>$\boldsymbol D\boldsymbol \theta = \boldsymbol c$</span>.</p><h1 id="Generation-Away-from-the-Data-Sites"><a class="docs-heading-anchor" href="#Generation-Away-from-the-Data-Sites">Generation Away from the Data Sites</a><a id="Generation-Away-from-the-Data-Sites-1"></a><a class="docs-heading-anchor-permalink" href="#Generation-Away-from-the-Data-Sites" title="Permalink"></a></h1><p>It is possible to extend these ideas so that we can approximate the derivative at any point <span>$\boldsymbol x_0 \in \mathcal C(\boldsymbol X)$</span>. Using the associated interpolant, simply approximate <span>$z_0$</span> with the value of the interpolant at <span>$\boldsymbol x_0$</span>, and then replace <span>$W_i$</span> by <span>$\lambda_i/\|\boldsymbol x_i-\boldsymbol x_0\|$</span>, where <span>$\lambda_i$</span> is the Sibson coordinate at <span>$\boldsymbol x_i$</span> relative to <span>$\boldsymbol x_0$</span>. If using a direct approach to approximate gradients and Hessians, Sibson coordinates cannot be used (because you can&#39;t extend the weights out to <span>$N_0^2$</span>) and so <span>$W_i$</span> remains as is in that case. Note that the <span>$N_0$</span> neighbourhoods are now the sets of natural neighbours.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../interpolation_math/">« Interpolation Details</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Tuesday 30 May 2023 11:19">Tuesday 30 May 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
